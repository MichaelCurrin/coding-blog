# Twitterverse project

>This article is about the purpose of and learnings around my [twitterverse]((https://github.com/MichaelCurrin/twitterverse)) repo.

I created my _twitterverse_ repo because I was interested in fetching, storing and reporting on data in the [Twitter API](https://developer.twitter.com/en/docs). Such as [trending topics](https://developer.twitter.com/en/docs/trends/trends-for-location/api-reference/get-trends-place) at locations, or using the [Search API](https://developer.twitter.com/en/docs/tweets/search/overview) to get individual tweets in the past 7 days which match a search query.

Some years ago I started out learning to use the [Twitter API](https://developer.twitter.com/en/docs) in the Twitter _developer console_ - that doesn't seem available anymore on their site, though [apigee](https://apigee.com/console) still provides this functionality. When I started learning Python, I found a way to compose URLs and do queries to get tweet data. Overtime, The requests I wanted to do become more complex and I needed an elegant way to handle rate-limiting. So, more recently, I moved using to the [tweepy](http://www.tweepy.org/) _Python_ library to access the Twitter API. There are also other _Python_ libraries out there like [Twython](https://twython.readthedocs.io/en/latest/).

Although I am aware of [SQLAlchemy](https://www.sqlalchemy.org/) and its popularity, I implemented this repo using [SQLObject](http://www.sqlobject.org/) as the [ORM](https://en.wikipedia.org/wiki/Object-relational_mapping) between Python and the database, since I was a lot more familiar with using _SQLObject_ at work and wanted to focus my efforts in one direction for a while. I like using an ORM to do complex [CRUD](https://en.wikipedia.org/wiki/Create,_read,_update_and_delete) operations on your database from within Python script or iPython console, since the ORM takes care of a lot of things for you like _JOIN_ operations. The downside, though, is the speed of execution, since reading and writing (whether to a text file or a database) is a lot slower than doing those operations in-memory. Also, the ORM has to process its own validation rules from within Python, which are not in the database schema. Also, SQL is implemented in famously fast _C_, while the ORM is running in _Python_ which has an interpreted language can never be as fast as the compiled C behind it.

I didn't realize how slow the ORM was compared with native _SQL_ until I started growing my project to large volumes of tweets. I used `tweepy` to do a search query on the Twitter API to get tweets within the past 7 days for a search query. I have a script to fetch the tweets (which includes the profiles of the authors), write the tweet and profile records and then assign each tweet a campaign label and each profile with a category label. The entire process took nearly 4 hours for 50 000 tweets. I wanted to do it faster.

I decided to optimize the campaign labeling process first. When assigning a campaign label, the script uses the ORM to insert a single record, get the record and then repeat for the remaining data items. The executions to read and write so many times is inefficient. So instead of handling record individually due to ORM limitation, I composed a single [INSERT](https://www.w3schools.com/sql/sql_insert.asp) statement in _SQL_ with all the required values. The duration came down from minutes to _less than a second_, which is several orders of magnitude faster. The ORM does have a [SQLBuilder](http://sqlobject.org/SQLBuilder.html) module which made this easy. In Python, I passed the `Insert` method a list of values and the table name, then it created the SQL statement.

Reducing the campaign insert time was only part of the script. I improved category insert the same way. And I worked out that inserting of the tweets and profile objects into the database should also get the same treatment, in order to reduce the entire insertion process to a matter of seconds. While fetch tweets from the API, I append thousands of rows to a CSV occasionally (either when hitting a max count of tweets or when hitting the rate limit window which means there is deadtime waiting which can be used). Then, when the entire process to fetch and write to a CSV finished, I can do the insert logic. This can be done immediately or when run manually later.

With the rate limiting of 480 pages per 15 minute window (using the [Application-only auth](https://developer.twitter.com/en/docs/basics/authentication/overview/application-only)) and a good network and processing speed, I am able to get almost 500 pages every 15 minutes, which is almost 2000 pages an hour. Ad one page has up to 100 tweets on it, letting me fetch nearly 200 000 tweets in a hour. That is 4 times the number of records as the previous case, but in a quarter of the time, meaning it is 8x faster. This makes it much easier to get large volumes of tweets into my system. I currently have this extract process working well and I am logging the requests to a log file. I can see that each request for 100 tweets takes on average close to 1 second on my machine. When executing on [PythonAnywhere](https://pythonanywhere.com) as a cloud solution, it only takes about a third of as second per request.
